# import os
# import cv2                              # X·ª≠ l√Ω h√¨nh ·∫£nh & video
# import torch                            # Framework deep learning (d√πng cho model ResNet)
# import mediapipe as mp                  # Ph√°t hi·ªán tay theo th·ªùi gian th·ª±c
# from torchvision import transforms
# from PIL import Image                   # X·ª≠ l√Ω ·∫£nh v·ªõi PIL
# import time

# # ‚îÄ‚îÄ‚îÄ C√°c thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng ƒë·ªÉ lo·∫°i b·ªè c√°c c·∫£nh b√°o n·ªôi b·ªô ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# # ƒê·∫∑t bi·∫øn m√¥i tr∆∞·ªùng ƒë·ªÉ TF Lite kh√¥ng d√πng feedback tensors (n·∫øu ƒë∆∞·ª£c h·ªó tr·ª£)
# os.environ["MEDIAPIPE_DISABLE_INFERENCE_FEEDBACK"] = "1"

# # N·∫øu c√†i absl-py, c√≥ th·ªÉ thi·∫øt l·∫≠p log level c·ªßa absl xu·ªëng ERROR
# try:
#     from absl import logging as absl_logging
#     absl_logging.set_verbosity(absl_logging.ERROR)
# except ImportError:
#     pass

# # ‚îÄ‚îÄ‚îÄ Fix SSL ch·ª©ng ch·ªâ ƒë·ªÉ t·∫£i d·ªØ li·ªáu n·∫øu c·∫ßn ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# import ssl, certifi
# ssl._create_default_https_context = lambda: ssl.create_default_context(cafile=certifi.where())

# # ‚îÄ‚îÄ‚îÄ Import model (ResNet, ASLDataset) ƒë√£ ƒë∆∞·ª£c vi·∫øt theo API m·ªõi ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# # ƒê·∫£m b·∫£o file model.py ƒë√£ thay ƒë·ªïi s·ª≠ d·ª•ng:
# #    resnet18(weights=ResNet18_Weights.DEFAULT)
# from model import ResNet, ASLDataset

# # ‚îÄ‚îÄ‚îÄ K·∫øt n·ªëi camera: ∆Øu ti√™n webcam m√°y t√≠nh, n·∫øu kh√¥ng m·ªü ƒë∆∞·ª£c th√¨ th·ª≠ IP cam ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# cap = cv2.VideoCapture(0)  # ∆Øu ti√™n webcam m√°y t√≠nh
# if not cap.isOpened():
#     print("[!] Kh√¥ng m·ªü ƒë∆∞·ª£c webcam m·∫∑c ƒë·ªãnh, th·ª≠ k·∫øt n·ªëi IP cam...")
#     IP_CAM_URL = 'http://192.168.0.102:8080/video'
#     cap = cv2.VideoCapture(IP_CAM_URL)
#     if not cap.isOpened():
#         print(f"[!] Kh√¥ng m·ªü ƒë∆∞·ª£c IP cam t·∫°i {IP_CAM_URL}. Tho√°t.")
#         exit(1)

# # ‚îÄ‚îÄ‚îÄ Kh·ªüi t·∫°o m√¥ h√¨nh AI v√† c√°c tham s·ªë
# num_classes = 29  # S·ªë l∆∞·ª£ng k√Ω t·ª± ASL + c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát
# model = ResNet(num_classes)
# model.load_state_dict(torch.load('best_model.pth', map_location='cpu'))
# model.eval()

# # Ti·ªÅn x·ª≠ l√Ω ·∫£nh
# preprocess = transforms.Compose([
#     transforms.Resize((128, 128)),
#     transforms.ToTensor(),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406],
#                          std=[0.229, 0.224, 0.225]),
# ])

# # ‚îÄ‚îÄ‚îÄ Kh·ªüi t·∫°o Mediapipe Hands v·ªõi c·∫•u h√¨nh m·ªõi ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# # Ch·∫ø ƒë·ªô static_image_mode=True s·∫Ω ch·∫°y ph√°t hi·ªán tr√™n m·ªói khung h√¨nh, ƒë·∫£m b·∫£o chu·∫©n h√¨nh ·∫£nh (v√† lo·∫°i b·ªè c·∫£nh b√°o v·ªÅ IMAGE_DIMENSIONS)
# mp_hands = mp.solutions.hands
# mp_drawing = mp.solutions.drawing_utils
# hands = mp_hands.Hands(
#     static_image_mode=False,        # Ch·∫°y detection tr√™n m·ªói frame
#     max_num_hands=2,
#     model_complexity=1,
#     min_detection_confidence=0.7,
#     min_tracking_confidence=0.5
# )

# # ‚îÄ‚îÄ‚îÄ H√†m chung l·∫•y ROI c·ªßa tay d·ª±a v√†o lo·∫°i tay ("Left" ho·∫∑c "Right") ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# def track_hand(frame, results, hand_type="left"):
#     frame_height, frame_width, _ = frame.shape
#     if not results.multi_handedness:
#         return None, None, None
#     hand_index = None
#     # Duy·ªát qua c√°c k·∫øt qu·∫£ ƒë·ªÉ t√¨m tay c√≥ label t∆∞∆°ng ·ª©ng
#     for idx, hand_handedness in enumerate(results.multi_handedness):
#         if hand_handedness.classification[0].label == hand_type:
#             hand_index = idx
#             break
#     if hand_index is None:
#         return None, None, None
#     hand_landmarks = results.multi_hand_landmarks[hand_index]
#     hand_center_x = int(hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_MCP].x * frame_width)
#     hand_center_y = int(hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_MCP].y * frame_height)
#     roi_size = 250  # ROI ƒë·∫£m b·∫£o h√¨nh vu√¥ng
#     top_left_x = max(0, hand_center_x - roi_size // 2)
#     top_left_y = max(0, hand_center_y - roi_size // 2)
#     bottom_right_x = min(frame_width, hand_center_x + roi_size // 2)
#     bottom_right_y = min(frame_height, hand_center_y + roi_size // 2)
#     roi = frame[top_left_y:bottom_right_y, top_left_x:bottom_right_x]
#     return roi, (top_left_x, top_left_y), (bottom_right_x, bottom_right_y)

# def track_left_hand(frame, results):
#     return track_hand(frame, results, hand_type="Left")

# def track_right_hand(frame, results):
#     return track_hand(frame, results, hand_type="Right")

# # ‚îÄ‚îÄ‚îÄ Bi·∫øn ƒë·ªÉ ch·ªçn ch·∫ø ƒë·ªô theo d√µi: False -> tay tr√°i, True -> tay ph·∫£i ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# use_right_hand = True  # ƒê·ªïi th√†nh True n·∫øu mu·ªën theo d√µi tay ph·∫£i

# # ‚îÄ‚îÄ‚îÄ Kh·ªüi t·∫°o c√°c bi·∫øn tr·∫°ng th√°i ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# flip_frame = False
# sentence = ""                   # Chu·ªói vƒÉn b·∫£n k·∫øt qu·∫£
# prev_label = None               # Nh√£n k√Ω hi·ªáu tr∆∞·ªõc ƒë√≥
# last_prediction_time = 0        # Th·ªùi gian nh·∫≠n d·∫°ng k√Ω t·ª± cu·ªëi c√πng
# prediction_delay = 5.0          # Nh·∫≠n 1 k√Ω t·ª± m·ªói 5 gi√¢y
# recording = False               # Tr·∫°ng th√°i ghi vƒÉn b·∫£n

# # ‚îÄ‚îÄ‚îÄ T·ª´ ƒëi·ªÉn √°nh x·∫° k√Ω hi·ªáu ASL ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# gestos = {
#     0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F',
#     6: 'G', 7: 'H', 8: 'I', 9: 'J', 10: 'K', 11: 'L',
#     12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R',
#     18: 'S', 19: 'T', 20: 'U', 21: 'V', 22: 'W', 23: 'X',
#     24: 'Y', 25: 'Z', 26: 'del', 27: 'nothing', 28: 'space'
# }

# # ‚îÄ‚îÄ‚îÄ V√≤ng l·∫∑p ch√≠nh x·ª≠ l√Ω h√¨nh ·∫£nh t·ª´ camera ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# while True:
#     ret, frame = cap.read()
#     if not ret:
#         print("Kh√¥ng nh·∫≠n ƒë∆∞·ª£c khung h√¨nh. Ki·ªÉm tra k·∫øt n·ªëi.")
#         break

#     if flip_frame:
#         frame = cv2.flip(frame, 1)

#     results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
#     current_time = time.time()

#     if results.multi_hand_landmarks:
#         # Ch·ªçn tay theo bi·∫øn use_right_hand
#         if use_right_hand:
#             roi, top_left, bottom_right = track_right_hand(frame, results)
#         else:
#             roi, top_left, bottom_right = track_left_hand(frame, results)
        
#         if roi is not None:
#             roi_pil = Image.fromarray(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB))
#             input_tensor = preprocess(roi_pil).unsqueeze(0)

#             with torch.no_grad():
#                 output = model(input_tensor)
#             pred = torch.argmax(output, dim=1).item()
#             label = gestos.get(pred, 'Unknown')

#             if recording:
#                 if label != prev_label and (current_time - last_prediction_time) > prediction_delay:
#                     if label == 'space':
#                         sentence += " "
#                     elif label == 'del':
#                         sentence = sentence[:-1]
#                     elif label != 'nothing':
#                         sentence += label

#                     prev_label = label
#                     last_prediction_time = current_time
#                     print("üìÑ VƒÉn b·∫£n hi·ªán t·∫°i:", sentence)

#             mp_drawing.draw_landmarks(frame, results.multi_hand_landmarks[0], mp_hands.HAND_CONNECTIONS)
#             cv2.rectangle(frame, top_left, bottom_right, (0, 255, 0), 2)
#             cv2.putText(frame, f"Label: {label}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX,
#                         1, (255, 255, 255), 2)

#     cv2.putText(frame, sentence, (10, 70), cv2.FONT_HERSHEY_SIMPLEX,
#                 1.2, (255, 255, 255), 2, cv2.LINE_AA)
#     cv2.imshow("Hand Tracking", frame)
    
#     key = cv2.waitKey(1) & 0xFF

#     if key == ord('b'):          # B·∫Øt ƒë·∫ßu ghi
#         recording = True
#         print(" B·∫ÆT ƒê·∫¶U GHI")
#     elif key == ord('d'):        # T·∫°m d·ª´ng ghi
#         recording = False
#         print("‚è∏ T·∫†M D·ª™NG")
#     elif key == ord('r'):        # Reset vƒÉn b·∫£n
#         sentence = ""
#         prev_label = None
#         print(" Reset c√¢u")
#     elif key == ord(' '):        # Th√™m d·∫•u c√°ch
#         sentence += " "
#         print(" Th√™m kho·∫£ng tr·∫Øng:", sentence)
#     elif key in (8, 127):        # X√≥a k√Ω t·ª± cu·ªëi (Backspace)
#         sentence = sentence[:-1]
#         print(" X√≥a k√Ω t·ª±, c√¢u hi·ªán t·∫°i:", sentence)
#     elif key == 27:              # Tho√°t
#         break

# cap.release()
# cv2.destroyAllWindows()

# import os
# import cv2
# import torch
# import mediapipe as mp
# import numpy as np
# from torchvision import transforms
# from PIL import Image
# import time
# import ssl, certifi

# # T·∫Øt c·∫£nh b√°o n·ªôi b·ªô c·ªßa MediaPipe
# os.environ["MEDIAPIPE_DISABLE_INFERENCE_FEEDBACK"] = "1"
# try:
#     from absl import logging as absl_logging
#     absl_logging.set_verbosity(absl_logging.ERROR)
# except ImportError:
#     pass
# ssl._create_default_https_context = lambda: ssl.create_default_context(cafile=certifi.where())

# # Import model
# from model import ResNet, ASLDataset

# # Camera
# cap = cv2.VideoCapture(0)
# if not cap.isOpened():
#     print("[!] Kh√¥ng m·ªü ƒë∆∞·ª£c webcam, th·ª≠ IP cam...")
#     IP_CAM_URL = 'http://192.168.0.102:8080/video'
#     cap = cv2.VideoCapture(IP_CAM_URL)
#     if not cap.isOpened():
#         print(f"[!] Kh√¥ng m·ªü IP cam t·∫°i {IP_CAM_URL}")
#         exit(1)

# # Model AI
# num_classes = 29
# model = ResNet(num_classes)
# model.load_state_dict(torch.load('best_model.pth', map_location='cpu'))
# model.eval()

# # Ti·ªÅn x·ª≠ l√Ω ·∫£nh
# preprocess = transforms.Compose([
#     transforms.Resize((128, 128)),
#     transforms.ToTensor(),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406],
#                          std=[0.229, 0.224, 0.225]),
# ])

# # MediaPipe
# mp_hands = mp.solutions.hands
# mp_drawing = mp.solutions.drawing_utils
# hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2,
#                        model_complexity=1, min_detection_confidence=0.7,
#                        min_tracking_confidence=0.5)

# mp_selfie_segmentation = mp.solutions.selfie_segmentation
# selfie_segmentation = mp_selfie_segmentation.SelfieSegmentation(model_selection=1)

# # H√†m tracking
# def track_hand(frame, results, hand_type="Left"):
#     frame_height, frame_width, _ = frame.shape
#     if not results.multi_handedness:
#         return None, None, None
#     hand_index = None
#     for idx, hand_handedness in enumerate(results.multi_handedness):
#         if hand_handedness.classification[0].label == hand_type:
#             hand_index = idx
#             break
#     if hand_index is None:
#         return None, None, None
#     hand_landmarks = results.multi_hand_landmarks[hand_index]
#     hand_center_x = int(hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_MCP].x * frame_width)
#     hand_center_y = int(hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_MCP].y * frame_height)
#     roi_size = 250
#     top_left_x = max(0, hand_center_x - roi_size // 2)
#     top_left_y = max(0, hand_center_y - roi_size // 2)
#     bottom_right_x = min(frame_width, hand_center_x + roi_size // 2)
#     bottom_right_y = min(frame_height, hand_center_y + roi_size // 2)
#     roi = frame[top_left_y:bottom_right_y, top_left_x:bottom_right_x]
#     return roi, (top_left_x, top_left_y), (bottom_right_x, bottom_right_y)

# track_left_hand = lambda frame, results: track_hand(frame, results, "Left")
# track_right_hand = lambda frame, results: track_hand(frame, results, "Right")

# # Tr·∫°ng th√°i
# use_right_hand = True
# flip_frame = False
# sentence = ""
# prev_label = None
# last_prediction_time = 0
# prediction_delay = 5.0
# recording = False

# gestos = {
#     0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F',
#     6: 'G', 7: 'H', 8: 'I', 9: 'J', 10: 'K', 11: 'L',
#     12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R',
#     18: 'S', 19: 'T', 20: 'U', 21: 'V', 22: 'W', 23: 'X',
#     24: 'Y', 25: 'Z', 26: 'del', 27: 'nothing', 28: 'space'
# }

# # Main loop
# while True:
#     ret, frame = cap.read()
#     if not ret:
#         print("Kh√¥ng nh·∫≠n ƒë∆∞·ª£c khung h√¨nh.")
#         break

#     if flip_frame:
#         frame = cv2.flip(frame, 1)

#     # L√†m m·ªù n·ªÅn
#     frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
#     results_seg = selfie_segmentation.process(frame_rgb)
#     condition = results_seg.segmentation_mask > 0.5
#     blurred_frame = cv2.GaussianBlur(frame, (55, 55), 0)
#     frame = np.where(condition[..., None], frame, blurred_frame)

#     results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
#     current_time = time.time()

#     if results.multi_hand_landmarks:
#         if use_right_hand:
#             roi, top_left, bottom_right = track_right_hand(frame, results)
#         else:
#             roi, top_left, bottom_right = track_left_hand(frame, results)

#         if roi is not None:
#             roi_pil = Image.fromarray(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB))
#             input_tensor = preprocess(roi_pil).unsqueeze(0)

#             with torch.no_grad():
#                 output = model(input_tensor)
#             pred = torch.argmax(output, dim=1).item()
#             label = gestos.get(pred, 'Unknown')

#             if recording:
#                 if label != prev_label and (current_time - last_prediction_time) > prediction_delay:
#                     if label == 'space':
#                         sentence += " "
#                     elif label == 'del':
#                         sentence = sentence[:-1]
#                     elif label != 'nothing':
#                         sentence += label
#                     prev_label = label
#                     last_prediction_time = current_time
#                     print("üìÑ VƒÉn b·∫£n hi·ªán t·∫°i:", sentence)

#             mp_drawing.draw_landmarks(frame, results.multi_hand_landmarks[0], mp_hands.HAND_CONNECTIONS)
#             cv2.rectangle(frame, top_left, bottom_right, (0, 255, 0), 2)
#             cv2.putText(frame, f"Label: {label}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX,
#                         1, (255, 255, 255), 2)

#     cv2.putText(frame, sentence, (10, 70), cv2.FONT_HERSHEY_SIMPLEX,
#                 1.2, (255, 255, 255), 2, cv2.LINE_AA)
#     cv2.imshow("Hand Tracking", frame)

#     key = cv2.waitKey(1) & 0xFF
#     if key == ord('b'):
#         recording = True
#         print("B·∫ÆT ƒê·∫¶U GHI")
#     elif key == ord('d'):
#         recording = False
#         print("‚è∏ T·∫†M D·ª™NG")
#     elif key == ord('r'):
#         sentence = ""
#         prev_label = None
#         print("Reset c√¢u")
#     elif key == ord(' '):
#         sentence += " "
#         print("Th√™m kho·∫£ng tr·∫Øng:", sentence)
#     elif key in (8, 127):
#         sentence = sentence[:-1]
#         print("X√≥a k√Ω t·ª±:", sentence)
#     elif key == 27:
#         break

# cap.release()
# cv2.destroyAllWindows()


import os
import cv2
import torch
import mediapipe as mp
from torchvision import transforms
from PIL import Image
import numpy as np
import time
import ssl, certifi

# Fix SSL ƒë·ªÉ t·∫£i model n·∫øu c·∫ßn
ssl._create_default_https_context = lambda: ssl.create_default_context(cafile=certifi.where())

# T·∫Øt c·∫£nh b√°o MediaPipe
os.environ["MEDIAPIPE_DISABLE_INFERENCE_FEEDBACK"] = "1"
try:
    from absl import logging as absl_logging
    absl_logging.set_verbosity(absl_logging.ERROR)
except ImportError:
    pass

from model import ResNet, ASLDataset

# K·∫øt n·ªëi camera
cap = cv2.VideoCapture(0)
if not cap.isOpened():
    IP_CAM_URL = 'http://192.168.0.102:8080/video'
    cap = cv2.VideoCapture(IP_CAM_URL)
    if not cap.isOpened():
        print("[!] Kh√¥ng m·ªü ƒë∆∞·ª£c camera.")
        exit(1)

# Load m√¥ h√¨nh
num_classes = 29
model = ResNet(num_classes)
model.load_state_dict(torch.load('best_model.pth', map_location='cpu'))
model.eval()

preprocess = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# MediaPipe setup v·ªõi ƒë·ªô nh·∫°y cao h∆°n
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=2,
    model_complexity=1,
    min_detection_confidence=0.75,   # tƒÉng ƒë·ªô nh·∫°y
    min_tracking_confidence=0.7      # tƒÉng gi·ªØ tay
)
selfie_segmentation = mp.solutions.selfie_segmentation.SelfieSegmentation(model_selection=1)

def track_hand(frame, results, hand_type="Left"):
    h, w, _ = frame.shape
    if not results.multi_handedness:
        return None, None, None
    hand_index = None
    for idx, handedness in enumerate(results.multi_handedness):
        if handedness.classification[0].label == hand_type:
            hand_index = idx
            break
    if hand_index is None:
        return None, None, None
    landmarks = results.multi_hand_landmarks[hand_index]
    cx = int(landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_MCP].x * w)
    cy = int(landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_MCP].y * h)
    roi_size = 350
    x1 = max(0, cx - roi_size // 2)
    y1 = max(0, cy - roi_size // 2)
    x2 = min(w, cx + roi_size // 2)
    y2 = min(h, cy + roi_size // 2)
    roi = frame[y1:y2, x1:x2]
    return roi, (x1, y1), (x2, y2)

track_left_hand = lambda frame, results: track_hand(frame, results, "Left")
track_right_hand = lambda frame, results: track_hand(frame, results, "Right")

# C·∫•u h√¨nh
use_right_hand = True
flip_frame = False
sentence = ""
prev_label = None
last_prediction_time = 0
prediction_delay = 5
recording = False

gestos = {i: c for i, c in enumerate(list("ABCDEFGHIJKLMNOPQRSTUVWXYZ") + ['del', 'nothing', 'space'])}

# V√≤ng l·∫∑p ch√≠nh
while True:
    ret, frame = cap.read()
    if not ret:
        print("Kh√¥ng nh·∫≠n ƒë∆∞·ª£c khung h√¨nh")
        break

    if flip_frame:
        frame = cv2.flip(frame, 1)

    # X√≥a n·ªÅn b·∫±ng l√†m m·ªù
    fg_mask = selfie_segmentation.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)).segmentation_mask
    condition = fg_mask > 0.5
    blurred = cv2.GaussianBlur(frame, (55, 55), 0)
    frame = np.where(condition[..., None], frame, blurred)

    results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    current_time = time.time()

    if results.multi_hand_landmarks:
        roi, top_left, bottom_right = track_right_hand(frame, results) if use_right_hand else track_left_hand(frame, results)
        if roi is not None:
            roi_pil = Image.fromarray(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB))
            input_tensor = preprocess(roi_pil).unsqueeze(0)
            with torch.no_grad():
                output = model(input_tensor)
            pred = torch.argmax(output, dim=1).item()
            label = gestos.get(pred, 'Unknown')

            if recording and label != prev_label and (current_time - last_prediction_time) > prediction_delay:
                if label == 'space':
                    sentence += " "
                elif label == 'del':
                    sentence = sentence[:-1]
                elif label != 'nothing':
                    sentence += label
                prev_label = label
                last_prediction_time = current_time
                print("üìÑ VƒÉn b·∫£n hi·ªán t·∫°i:", sentence)

            mp_drawing.draw_landmarks(frame, results.multi_hand_landmarks[0], mp_hands.HAND_CONNECTIONS)
            cv2.rectangle(frame, top_left, bottom_right, (0, 255, 0), 2)

            # Hi·ªÉn th·ªã nh√£n c√≥ vi·ªÅn ƒëen
            cv2.putText(frame, f"Label: {label}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX,
                        1, (0, 0, 0), 4, cv2.LINE_AA)
            cv2.putText(frame, f"Label: {label}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX,
                        1, (255, 255, 255), 2, cv2.LINE_AA)

    # Hi·ªÉn th·ªã vƒÉn b·∫£n c√≥ vi·ªÅn ƒëen
    cv2.putText(frame, sentence, (10, 70), cv2.FONT_HERSHEY_SIMPLEX,
                1.2, (0, 0, 0), 4, cv2.LINE_AA)
    cv2.putText(frame, sentence, (10, 70), cv2.FONT_HERSHEY_SIMPLEX,
                1.2, (255, 255, 255), 2, cv2.LINE_AA)

    cv2.imshow("Hand Tracking", frame)

    key = cv2.waitKey(1) & 0xFF
    if key == ord('b'):
        recording = True
        print(" B·∫ÆT ƒê·∫¶U GHI")
    elif key == ord('d'):
        recording = False
        print("‚è∏ T·∫†M D·ª™NG")
    elif key == ord('r'):
        sentence = ""
        prev_label = None
        print(" Reset c√¢u")
    elif key == ord(' '):
        sentence += " "
        print(" Th√™m kho·∫£ng tr·∫Øng:", sentence)
    elif key in (8, 127):
        sentence = sentence[:-1]
        print(" X√≥a k√Ω t·ª±, c√¢u hi·ªán t·∫°i:", sentence)
    elif key == 27:
        break

cap.release()
cv2.destroyAllWindows()




